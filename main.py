from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import time
import sys
import os
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="SmartHotel AI Service", version="1.0.0")

# Model state tracking
model_state = {
    "loaded": False,
    "loading": False,
    "error": None
}

class AnalyzeRequest(BaseModel):
    text: str

class AnalyzeResponse(BaseModel):
    original_text: str
    processed_text: Optional[str]
    sentiment_score: float
    sentiment_label: str
    confidence: Optional[float]
    model_version: Optional[str]
    processing_time_ms: Optional[int]
    keywords: Optional[List[str]]

class AnalyzeSingleRequest(BaseModel):
    review_text: str

class AnalyzeSingleResponse(BaseModel):
    review_text: str
    processed_text: Optional[str]
    predicted_rating: float
    sentiment_label: Optional[str]
    confidence: Optional[float]
    model_version: Optional[str]
    processing_time_ms: Optional[int]

async def ensure_model_loaded():
    """ƒê·∫£m b·∫£o model ƒë∆∞·ª£c load tr∆∞·ªõc khi s·ª≠ d·ª•ng"""
    if model_state["loaded"]:
        return True
    
    if model_state["loading"]:
        # Ch·ªù model load xong
        import asyncio
        while model_state["loading"]:
            await asyncio.sleep(0.1)
        return model_state["loaded"]
    
    # Load model
    model_state["loading"] = True
    try:
        logger.info("üîÑ Loading BERT model on demand...")
        
        # Import v√† kh·ªüi t·∫°o model service
        sys.path.append(os.path.join(os.path.dirname(__file__), "service"))
        from service.bert_model_service import initialize_model
        
        # Initialize model
        initialize_model()
        
        model_state["loaded"] = True
        model_state["error"] = None
        logger.info("‚úÖ Model loaded successfully!")
        
    except Exception as e:
        error_msg = f"Failed to load model: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        model_state["error"] = error_msg
        model_state["loaded"] = False
        
    finally:
        model_state["loading"] = False
    
    return model_state["loaded"]

@app.post("/analyze", response_model=AnalyzeResponse)
async def analyze_sentiment(req: AnalyzeRequest):
    # ƒê·∫£m b·∫£o model ƒë√£ load
    if not await ensure_model_loaded():
        raise HTTPException(
            status_code=503, 
            detail=f"Model unavailable: {model_state['error']}"
        )
    
    start = time.time()
    
    try:
        # Import function khi c·∫ßn
        from service.bert_model_service import predict_review_rating_bert
        
        # S·ª≠ d·ª•ng BERT model
        result = predict_review_rating_bert(req.text)

        # Tr√≠ch xu·∫•t keywords t·ª´ text (ƒë∆°n gi·∫£n)
        processed_text = result.get("processed_text", req.text.lower())
        keywords = processed_text.split()[:5] if processed_text else []

        processing_time_ms = int((time.time() - start) * 1000)

        return AnalyzeResponse(
            original_text=req.text,
            processed_text=processed_text,
            sentiment_score=result.get("predicted_rating"),
            sentiment_label=result.get("sentiment_label"),
            confidence=result.get("confidence"),
            model_version=result.get("model_version"),
            processing_time_ms=processing_time_ms,
            keywords=keywords,
        )
        
    except Exception as e:
        logger.error(f"‚ùå Error in analyze_sentiment: {e}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@app.post("/analyze-single", response_model=AnalyzeSingleResponse)
async def analyze_single_review(req: AnalyzeSingleRequest):
    # ƒê·∫£m b·∫£o model ƒë√£ load
    if not await ensure_model_loaded():
        raise HTTPException(
            status_code=503, 
            detail=f"Model unavailable: {model_state['error']}"
        )
    
    start = time.time()
    
    try:
        # Import function khi c·∫ßn
        from service.bert_model_service import predict_review_rating_bert
        
        # S·ª≠ d·ª•ng BERT model
        result = predict_review_rating_bert(req.review_text)
        processing_time_ms = int((time.time() - start) * 1000)
        
        return AnalyzeSingleResponse(
            review_text=req.review_text,
            processed_text=result.get("processed_text"),
            predicted_rating=result.get("predicted_rating"),
            sentiment_label=result.get("sentiment_label"),
            confidence=result.get("confidence"),
            model_version=result.get("model_version"),
            processing_time_ms=processing_time_ms,
        )
        
    except Exception as e:
        logger.error(f"‚ùå Error in analyze_single_review: {e}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@app.get("/health")
async def health_check():
    """Health check endpoint - kh√¥ng ph·ª• thu·ªôc v√†o model"""
    return {
        "status": "healthy",
        "service": "SmartHotel AI Service",
        "version": "1.0.0",
        "timestamp": "2025-06-15",
        "port": os.environ.get("PORT", "8000")
    }

@app.get("/model-status")
async def model_status():
    """Check model loading status"""
    return {
        "model_loaded": model_state["loaded"],
        "model_loading": model_state["loading"],
        "error": model_state["error"],
        "status": "ready" if model_state["loaded"] else "not_loaded"
    }

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "SmartHotel AI Service is running",
        "model_status": "loaded" if model_state["loaded"] else "not_loaded",
        "endpoints": ["/analyze", "/analyze-single", "/health", "/model-status"]
    }

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    
    logger.info(f"üöÄ Starting SmartHotel AI Service on 0.0.0.0:{port}")
    logger.info("‚è≥ Model s·∫Ω ƒë∆∞·ª£c load khi c√≥ request ƒë·∫ßu ti√™n")
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=False
    )


"""
ƒê·ªÉ ch·∫°y ch∆∞∆°ng tr√¨nh v√† s·ª≠ d·ª•ng c√°c API x·ª≠ l√Ω AI:

1. ƒê·∫£m b·∫£o ƒë√£ c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt:
   ```bash
   pip install fastapi uvicorn transformers torch scikit-learn
   ```

2. Ch·∫°y FastAPI b·∫±ng Uvicorn:
   ```bash
   uvicorn main:app --reload
   ```
   (Ch·∫°y l·ªánh n√†y trong th∆∞ m·ª•c ch·ª©a file `main.py`.)

3. Sau khi ch·∫°y, API s·∫Ω l·∫Øng nghe t·∫°i ƒë·ªãa ch·ªâ:
   ```
   http://localhost:8000
   ```

4. ƒê·ªÉ ki·ªÉm tra/t∆∞∆°ng t√°c v·ªõi API, truy c·∫≠p:
   ```
   http://localhost:8000/docs
   ```
   (Swagger UI t·ª± ƒë·ªông sinh.)

5. G·ª≠i request POST t·ªõi c√°c endpoint nh∆∞ `/analyze-single` ƒë·ªÉ ph√¢n t√≠ch b√¨nh lu·∫≠n.

**L∆∞u √Ω:**  
- ƒê·∫£m b·∫£o BERT model v√† th∆∞ m·ª•c `service` ƒë√£ ƒë√∫ng v·ªã tr√≠.
- Model s·∫Ω t·ª± ƒë·ªông download n·∫øu ch∆∞a c√≥ local.
- N·∫øu g·∫∑p l·ªói thi·∫øu file ho·∫∑c th∆∞ vi·ªán, ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n v√† c√†i ƒë·∫∑t.
"""
